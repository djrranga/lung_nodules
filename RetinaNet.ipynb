{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7638fbd-92cd-42ce-93ff-5d80cafa7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66110630-292a-4c0a-9fdc-be7e6b87bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaPatchRetinaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that:\n",
    "    - reads patch images from patches/\n",
    "    - reads true nodules from annotations.csv\n",
    "    - matches by seriesuid + 3D distance\n",
    "    - returns (image, target) for RetinaNet\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_dir=\"patches\", annotations_csv=\"annotations.csv\", transforms=None):\n",
    "        self.patches_dir = patches_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # 1) list all patch images\n",
    "        self.img_paths = sorted(\n",
    "            [\n",
    "                os.path.join(patches_dir, f)\n",
    "                for f in os.listdir(patches_dir)\n",
    "                if f.lower().endswith(\".tiff\") or f.lower().endswith(\".tif\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if len(self.img_paths) == 0:\n",
    "            raise RuntimeError(f\"No .tiff patches found in {patches_dir}\")\n",
    "\n",
    "        # 2) load LUNA16 annotations\n",
    "        # IMPORTANT: in your folder it's just \"annotations.csv\"\n",
    "        df = pd.read_csv(annotations_csv)\n",
    "\n",
    "        # 3) group nodules by seriesuid\n",
    "        self.nodules_by_uid = {}\n",
    "        for _, row in df.iterrows():\n",
    "            uid = str(row[\"seriesuid\"])\n",
    "            self.nodules_by_uid.setdefault(uid, []).append(\n",
    "                {\n",
    "                    \"x\": float(row[\"coordX\"]),\n",
    "                    \"y\": float(row[\"coordY\"]),\n",
    "                    \"z\": float(row[\"coordZ\"]),\n",
    "                    \"d\": float(row[\"diameter_mm\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def _parse_filename(self, path):\n",
    "        \"\"\"\n",
    "        Expecting: patch_<uid>_<x>_<y>_<z>.tiff\n",
    "        \"\"\"\n",
    "        name = os.path.basename(path)\n",
    "        # drop extension\n",
    "        if name.lower().endswith(\".tiff\"):\n",
    "            name = name[:-5]\n",
    "        else:\n",
    "            name = os.path.splitext(name)[0]\n",
    "\n",
    "        parts = name.split(\"_\", 4)\n",
    "        if len(parts) != 5:\n",
    "            raise ValueError(f\"Unexpected patch name format: {name}\")\n",
    "        _, uid, wx, wy, wz = parts\n",
    "        return uid, float(wx), float(wy), float(wz)\n",
    "\n",
    "    def _match_nodule(self, uid, wx, wy, wz):\n",
    "        \"\"\"\n",
    "        Return matched nodule dict if candidate is close enough.\n",
    "        \"\"\"\n",
    "        nods = self.nodules_by_uid.get(uid, [])\n",
    "        for n in nods:\n",
    "            dx = wx - n[\"x\"]\n",
    "            dy = wy - n[\"y\"]\n",
    "            dz = wz - n[\"z\"]\n",
    "            dist = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "            if dist <= n[\"d\"] / 2.0:\n",
    "                return n\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        uid, wx, wy, wz = self._parse_filename(img_path)\n",
    "\n",
    "        # load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        match = self._match_nodule(uid, wx, wy, wz)\n",
    "\n",
    "        if match is not None:\n",
    "            # positive example → put 1 box in center\n",
    "            approx_pix = int(round(match[\"d\"]))  # crude mm→px\n",
    "            box_size = max(8, min(approx_pix, min(w, h) - 2))\n",
    "            cx, cy = w / 2.0, h / 2.0\n",
    "            x1 = cx - box_size / 2.0\n",
    "            y1 = cy - box_size / 2.0\n",
    "            x2 = cx + box_size / 2.0\n",
    "            y2 = cy + box_size / 2.0\n",
    "            boxes = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "            labels = torch.tensor([1], dtype=torch.int64)\n",
    "        else:\n",
    "            # negative example → no boxes\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc3dbd6-7285-4d14-a83e-ea77c5d386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf00d3f7-c506-43b2-9d07-63c5d8412db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes=2):\n",
    "    # For torchvision >= 0.14 style\n",
    "    model = retinanet_resnet50_fpn_v2(\n",
    "        weights=None,          # start from scratch\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed188a2-edde-429c-9995-6c8a1c7112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    patches_dir = \"patches\"\n",
    "    annotations_csv = \"annotations.csv\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "    dataset = LunaPatchRetinaDataset(\n",
    "        patches_dir=patches_dir,\n",
    "        annotations_csv=annotations_csv,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "\n",
    "    val_ratio = 0.1\n",
    "    n_total = len(dataset)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_train = n_total - n_val\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,      # ← important on Mac / notebook\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,      # ← important\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    model = get_model(num_classes=2).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - train loss: {loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                preds = model(images)\n",
    "                break\n",
    "        print(\"Ran validation forward pass ✅\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"retinanet_luna_patches.pth\")\n",
    "    print(\"Model saved to retinanet_luna_patches.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ebc52ab-5d03-4088-873e-c0e2b5dcf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "   # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519fb78e-06c9-495d-9379-63f38b407ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/andy/Documents/Schoolwork/2025-2026/BMME 575/ML_project\n",
      "here: ['RetinaNet.ipynb', '.DS_Store', 'Preprocessor.ipynb', 'patches', 'SimpleITK Tutorial(1)(1).ipynb', '.ipynb_checkpoints', 'annotations.csv', 'subset0', 'candidates_V2.csv']\n",
      "annotations shape: (1186, 5)\n",
      "patches: 79135\n",
      "['patch_1.3.6.1.4.1.14519.5.2.1.6279.6001.210837812047373739447725050963_-263.43_-80.75_-93.55.tiff', 'patch_1.3.6.1.4.1.14519.5.2.1.6279.6001.657775098760536289051744981056_-161.22_58.23_69.25.tiff', 'patch_1.3.6.1.4.1.14519.5.2.1.6279.6001.716498695101447665580610403574_-181.19_-112.83_47.07.tiff', 'patch_1.3.6.1.4.1.14519.5.2.1.6279.6001.525937963993475482158828421281_-72.02_57.39_47.44.tiff', 'patch_1.3.6.1.4.1.14519.5.2.1.6279.6001.194440094986948071643661798326_-98.80_-165.46_-64.19.tiff']\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"here:\", os.listdir(\".\"))\n",
    "\n",
    "df = pd.read_csv(\"annotations.csv\")\n",
    "print(\"annotations shape:\", df.shape)\n",
    "\n",
    "print(\"patches:\", len(os.listdir(\"patches\")))\n",
    "print(os.listdir(\"patches\")[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bfcdea2-fe7c-49b8-b7ad-a0e6c44ed745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: torch.Size([3, 65, 65])\n",
      "target: {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'image_id': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([T.ToTensor()])\n",
    "ds = LunaPatchRetinaDataset(\n",
    "    patches_dir=\"patches\",\n",
    "    annotations_csv=\"annotations.csv\",\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "img, target = ds[0]\n",
    "print(\"img shape:\", img.shape)\n",
    "print(\"target:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d039cbdd-7fb0-4997-b3a6-e91e9d88f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "torch.Size([3, 65, 65]) {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'image_id': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,     # <- keep 0 on Mac/notebook\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "images, targets = batch\n",
    "print(len(images), len(targets))\n",
    "print(images[0].shape, targets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c194678-9d8d-4df3-8166-e16e50c83e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification': tensor(0.3882, grad_fn=<DivBackward0>), 'bbox_regression': tensor(0., grad_fn=<DivBackward0>)}\n",
      "total loss: 0.38817182183265686\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model(num_classes=2).to(device)\n",
    "\n",
    "images, targets = next(iter(loader))   # from your dataloader with num_workers=0\n",
    "images = [img.to(device) for img in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.train()\n",
    "loss_dict = model(images, targets)\n",
    "loss = sum(loss_dict.values())\n",
    "print(loss_dict)\n",
    "print(\"total loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d98ed0f-0080-4da3-96b3-bb1d0ca6a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one training step ✅\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"one training step ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a1793-fc55-40a1-879a-2fe4ad97ac73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
